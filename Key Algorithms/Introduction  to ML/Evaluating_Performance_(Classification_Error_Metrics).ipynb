{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation\n",
        "\n",
        "## Typically in any classification task your model can only achieve two results:\n",
        "  \n",
        "  1.Either your model was correct in its prediction.\n",
        "  \n",
        "  2.Or your model was incorrect in its prediction.\n",
        "\n",
        "## Fortunately incorrect vs correct expands to situations where you have multiple classes.\n",
        "\n",
        "## For the purposes of explaining the metrics, let's imagine a binary classification situation,where we only have two available classes.\n",
        "\n",
        "\n",
        "In our example, we will attempt to predict if an image is a dog or a cat.\n",
        "\n",
        "Since this is supervised learning, we will first fit/train a model on traing data, then test the model on testing data.\n",
        "\n",
        "Once we have the model's predictions from the X_test data, we compare it to the true y values(the correct labels)."
      ],
      "metadata": {
        "id": "MIquBLA2kT_F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We repeat this process for all the images in ou X test data.\n",
        "\n",
        "At the end we will have a count of correct matches and count of incorrect matches.\n",
        "\n",
        "The key realization we need to make, is that in the real world, not all incorrect or correct matches hold equal value.\n",
        "\n",
        "Also in the real world, a single matric won't tell the complete story.\n",
        "\n",
        "To understand all of this, let's bring back the 4 metrics we mntioned and see how they are calculated.\n",
        "\n",
        "We could organize our predicted values compared to the real values in a confusion matrix."
      ],
      "metadata": {
        "id": "fhtQWodYwVBz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Accuracy\n",
        "\n",
        "Accuracy in classification problems is the number of correct predictions made by the model divided by the total number of predictions.\n",
        "\n",
        "For example, if the X_test set was 100 images and our model correctly predicted 80 images, then we have 80/100.\n",
        "0.8 or 80% accuracy.\n",
        "\n",
        "Accuracy is usefull when target classes are well balanced\n",
        "\n",
        "In our example, we would have roughly the same amount off cat images as we have dog images.\n",
        "\n",
        "Accuracy is not a good choice with unbalanced classes.\n",
        "\n",
        "Imagine we had 99 images of dogs and 1 image of a cat.\n",
        "\n",
        "If our model was simply a line that always predicted dog we would get 99% accuracy.\n",
        "\n"
      ],
      "metadata": {
        "id": "CxEyWJa5x6tj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recall\n",
        "\n",
        "Ability of a model to find all the relevant cases within a dataset.\n",
        "\n",
        "The precise definition of recall is the number true positives divides divided by the number of true positives plus the number of false negatives.\n",
        "\n"
      ],
      "metadata": {
        "id": "M1vwUlq84cwI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Precision\n",
        "\n",
        "Ability of a classification model to identify only the relevant data points.\n",
        "\n",
        "Precision is defined as the number od true positives divided by the number of true positives plus the number of false positives.\n"
      ],
      "metadata": {
        "id": "ThNf7mra7kKU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recall and Precision\n",
        "\n",
        "Often you have a trade-off between Recall and Precision.\n",
        "\n",
        "While recall expresses the ability to find all relevant instances in a dataset,precision expresses the proportion of the data points our model says was relevant actually were relevant."
      ],
      "metadata": {
        "id": "0SPTru8v8nwM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# F1-Score\n",
        "\n",
        "In cases where we want to find an optimal blend of precision and recall we can combine the two metrics using what is called the F1-Score.\n",
        "\n",
        "\n",
        "The F1-Score is the harmonic mean of precision and recall taking both metrics into account in the following equation:\n",
        "\n",
        "F1 = 2*((precision * recall)/(precision+recall))\n",
        "\n",
        "\n",
        "We use the harmonic mean instead of a simple average because it punishes extreme values.\n",
        "\n",
        "A classifier with a precission of 1.0 and a recall of 0.0 has a single average of 0.5 but an F1-Score of 0.\n"
      ],
      "metadata": {
        "id": "XRNFkmz49b7s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main point to remember with the confusion matrix and the various calculated metrics is that they are all fundamentally ways of comparing the predicted values versus the true values.\n",
        "\n",
        "\n",
        "What constitutes \"good\" metrics, will really depend on the specific situation.\n",
        "\n",
        "Often models are used as as quick diagnostic tests to have before having a more invasive test(e.g. getting urine test before getting a biopsy)\n",
        "\n",
        "We also need to consider what is at stake.\n",
        "\n",
        "Often we have a precision/recall trade off, We need to decide if the model will should focus on fixing False Positives vs. False Negatives.\n",
        "\n",
        "In disease diagnosis, it is probably better to go in the direction of False positives, so we make sure we correctly classify as many cases of disease as possible.\n",
        "\n",
        "All of this is to say, machine learning is not performed in a \"vacuum\", but insted a collaborative process where we should consult with experts in the domain(e.g. medical doctors)"
      ],
      "metadata": {
        "id": "dF672STQmZ0N"
      }
    }
  ]
}