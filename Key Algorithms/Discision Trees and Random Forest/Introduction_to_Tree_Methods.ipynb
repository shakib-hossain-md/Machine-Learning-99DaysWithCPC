{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Imagine that I play Tennis every satarday and I always invite a friend to come with me.\n",
        "\n",
        "Sometimes my friend shows up,sometimes not.\n",
        "\n",
        "For him it depends on a variety of factors, such as: weather, temperature,humidity,wind etc...\n",
        "\n",
        "I start keeping track of these features and whether or not he showed up to play with me."
      ],
      "metadata": {
        "id": "va72jaUbV0R6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I want to use this data to predict whether or not he will show up to play.\n",
        "\n",
        "An intuitive way to do this is through a Decision Tree.\n",
        "\n",
        "\n",
        "In this Tree we have:\n",
        "\n",
        "1. Nodes- split for the value of a certain attribute.\n",
        "\n",
        "2. Edges-Outcome of a split to next node\n",
        "\n",
        "3. Root- The node that performs the first split\n",
        "\n",
        "4. Leaves- Terminal nodes that predict the outcome."
      ],
      "metadata": {
        "id": "xqhUtKJHWzen"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forests\n",
        "\n",
        "---\n",
        "To improve performance, we can use many trees with a random sample of features chosen as tha split.\n",
        "\n",
        "1. A new random sample of features is chosen for every single tree at every single split.\n",
        "\n",
        "2. For classification, m is typically chosen to be tha square root of p.\n",
        "\n",
        "\n",
        "What's the point?\n",
        "\n",
        "Suppose there is one very strong feature in the data set. When using \"bagged\" trees, most o the trees will use that feature as the top split, resulting in an ensemble of similar trees that are highly correlated.\n",
        "\n",
        "\n",
        "Averaging highly correlated quantities does not significantly reduce variance.\n",
        "\n",
        "By randomly leaving out candidate features from each split, Random Forest \"decorrelates\"the trees, such that the averaging process can reduce the variance of the resulting model."
      ],
      "metadata": {
        "id": "IEvE2glXYZUG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wx7LzCg7VEpf"
      },
      "outputs": [],
      "source": []
    }
  ]
}